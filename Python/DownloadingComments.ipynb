{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0352638c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rameshkumar/Desktop/untitled folder 2/Python/ModelTraining.py:45: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  parse_obj=BeautifulSoup(text,'html.parser')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mModelTraining\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/untitled folder 2/Python/ModelTraining.py:126\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# ## Tf-idf Vectorizer\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# In[20]:\u001b[39;00m\n\u001b[1;32m    125\u001b[0m tfidf_obj\u001b[38;5;241m=\u001b[39mTfidfVectorizer(min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m--> 126\u001b[0m tf_train_reviews\u001b[38;5;241m=\u001b[39mtfidf_obj\u001b[38;5;241m.\u001b[39mfit_transform(train_review)\n\u001b[1;32m    127\u001b[0m tf_test_reviews\u001b[38;5;241m=\u001b[39mtfidf_obj\u001b[38;5;241m.\u001b[39mtransform(test_review)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# ## MultiNomial Naive Bayes\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# In[21]:\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2133\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2128\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2129\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2130\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2131\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2132\u001b[0m )\n\u001b[0;32m-> 2133\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[1;32m   2134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2135\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2136\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1405\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1401\u001b[0m     X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_words_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limit_features(\n\u001b[1;32m   1402\u001b[0m         X, vocabulary, max_doc_count, min_doc_count, max_features\n\u001b[1;32m   1403\u001b[0m     )\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1405\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n\u001b[1;32m   1406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_ \u001b[38;5;241m=\u001b[39m vocabulary\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1208\u001b[0m, in \u001b[0;36mCountVectorizer._sort_features\u001b[0;34m(self, X, vocabulary)\u001b[0m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sort_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, vocabulary):\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sort features by name\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \n\u001b[1;32m   1206\u001b[0m \u001b[38;5;124;03m    Returns a reordered matrix and modifies the vocabulary in place\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1208\u001b[0m     sorted_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(vocabulary\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m   1209\u001b[0m     map_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(sorted_features), dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m new_val, (term, old_val) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sorted_features):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ModelTraining "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d13d88",
   "metadata": {},
   "source": [
    "# Model is Ready Now lets download the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6917223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeba8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "# Set your API key here\n",
    "API_KEY = 'AIzaSyALFyd37NGnUiYFGFhmQREhRKugsMHCXDA'\n",
    "\n",
    "# YouTube Data API service\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "def get_video_comments(video_id):\n",
    "    try:\n",
    "        comments = []\n",
    "        next_page_token = None\n",
    "\n",
    "        while True:\n",
    "            response = youtube.commentThreads().list(\n",
    "                part='snippet',\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token\n",
    "            ).execute()\n",
    "\n",
    "            for item in response['items']:\n",
    "                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                comments.append(comment)\n",
    "\n",
    "            next_page_token = response.get('nextPageToken')\n",
    "\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        return comments\n",
    "    except HttpError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_comments_to_csv(comments, csv_filename):\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['Comment'])\n",
    "        for comment in comments:\n",
    "            writer.writerow([comment])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_id = 'vhwr4vc_GY0'\n",
    "    csv_filename = 'comments.csv'\n",
    "\n",
    "    comments = get_video_comments(video_id)\n",
    "    save_comments_to_csv(comments, csv_filename)\n",
    "\n",
    "    print(f\"Comments saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be92db",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df=pd.read_csv('comments.csv')\n",
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32738a49",
   "metadata": {},
   "source": [
    "## Preprocessing the dowloaded comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e042369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df['Comment']=comments_df['Comment'].apply(clean_data)\n",
    "comments_df['Comment']=comments_df['Comment'].apply(porter_stemmer)\n",
    "comments_df['Comment']=comments_df['Comment'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5135b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37709fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=tfidf_obj.transform(comments_df['Comment'])\n",
    "pred=MNB.predict(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53755208",
   "metadata": {},
   "outputs": [],
   "source": [
    "value,count=np.unique(pred,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2a54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c20fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
